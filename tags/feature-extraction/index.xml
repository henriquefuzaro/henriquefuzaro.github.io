<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Feature Extraction on André Sbrocco Figueiredo</title>
    <link>https://andresbrocco.github.io/tags/feature-extraction/</link>
    <description>Recent content in Feature Extraction on André Sbrocco Figueiredo</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>=&amp;copy; 2019</copyright>
    <lastBuildDate>Wed, 27 Feb 2019 00:00:00 +0000</lastBuildDate>
    
	    <atom:link href="https://andresbrocco.github.io/tags/feature-extraction/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Audio-visuals in shared space as a metaphor for mindscapes</title>
      <link>https://andresbrocco.github.io/project/audiovisual-in-shared-space-as-a-metaphor-for-mindscapes/</link>
      <pubDate>Wed, 27 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://andresbrocco.github.io/project/audiovisual-in-shared-space-as-a-metaphor-for-mindscapes/</guid>
      <description>&lt;p&gt;The Group &amp;lsquo;Perception, Action, and Interaction&amp;rsquo; at &lt;a href=&#34;https://www.nics.unicamp.br/&#34;&gt;NICS&lt;/a&gt; explores the interaction between humans and digital media; &amp;lsquo;Perception&amp;rsquo; stands for audio analysis, psychoacoustics and visual cognition; &amp;lsquo;Action&amp;rsquo; stands for designing sound and visual arts; &amp;lsquo;Interaction&amp;rsquo; stands for how humans interfaces with the digital environment.&lt;/p&gt;

&lt;video width=&#34;320&#34; height=&#34;240&#34; controls&gt;
  &lt;source src=&#34;https://andresbrocco.github.io/project/audiovisual-in-shared-space-as-a-metaphor-for-mindscapes/Audio-visuals_in_shared_space_as_a_metaphor_for_mindscapes.mp4&#34; type=&#34;video/mp4&#34;&gt;
Your browser does not support the video tag.
&lt;/video&gt; 

&lt;p&gt;The presented work is a prototype of the first project of the group. The concept is to build an interactive imersive environment where a performer can navigate through a sphere full of silent movie clips and interact with them. The navigation is controlled by the users hands and position in the room, while other body features control video and audio effects.&lt;/p&gt;

&lt;p&gt;The audio is also composed in real time using granular synthesis and mixing musical themes associated with each clip. This combination creates a noisy and thematic atmosphere that resembles an antique silent movie cinema. A shared space is created using the network, so that many actors can perform in real time, each one from its own machine. This process is documented at &lt;a href=&#34;https://andresbrocco.github.io/publication/audio-visuals-in-shared-space-as-a-metaphor-for-mindscapes/&#34;&gt;this publication&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kinect Skeleton library for Processing</title>
      <link>https://andresbrocco.github.io/project/kinect-skeleton-library-for-processing/</link>
      <pubDate>Wed, 27 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://andresbrocco.github.io/project/kinect-skeleton-library-for-processing/</guid>
      <description>&lt;p&gt;The Group &amp;lsquo;Perception, Action, and Interaction&amp;rsquo; at &lt;a href=&#34;https://www.nics.unicamp.br/&#34;&gt;NICS&lt;/a&gt; explores the interaction between humans and digital media; &amp;lsquo;Perception&amp;rsquo; stands for audio analysis, psychoacoustics and visual cognition; &amp;lsquo;Action&amp;rsquo; stands for designing sound and visual arts; &amp;lsquo;Interaction&amp;rsquo; stands for how humans interfaces with the digital environment. One ongoing research example is the sonification of human body movement with application purposes in music therapy.&lt;/p&gt;

&lt;p&gt;One of the devices used by the laboratory to sense the body movements is the &lt;a href=&#34;https://developer.microsoft.com/en-us/windows/kinect&#34;&gt;Microsoft Kinect&lt;/a&gt;. Therefore, some projects rely on a robust tool to preprocess the incoming data and extract body movement features. My role is to develop that tool and make it available as an user friendly software for people with low level of knowledge in software development.&lt;/p&gt;

&lt;p&gt;The main features implemented in the library available on my &lt;a href=&#34;https://github.com/andresbrocco/Processing_KinectV2_SkeletonTools&#34; &gt;github&lt;/a&gt; are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Interface with KinectV2&lt;/li&gt;
&lt;li&gt;Smooth skeleton&lt;/li&gt;
&lt;li&gt;Calibrate for the floor position and room size&lt;/li&gt;
&lt;li&gt;Extract body features&lt;/li&gt;
&lt;li&gt;Send features through network via OSC&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The video below is a brief demonstration of the library and its features.&lt;/p&gt;

&lt;video width=&#34;320&#34; height=&#34;240&#34; controls&gt;
  &lt;source src=&#34;https://andresbrocco.github.io/project/kinect-skeleton-library-for-processing/KinectV2SkeletonToolsDemo.mp4&#34; type=&#34;video/mp4&#34;&gt;
Your browser does not support the video tag.
&lt;/video&gt; 
</description>
    </item>
    
    <item>
      <title>Chroma Jammer</title>
      <link>https://andresbrocco.github.io/project/chroma-jammer/</link>
      <pubDate>Wed, 27 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://andresbrocco.github.io/project/chroma-jammer/</guid>
      <description>&lt;p&gt;ChromaJammer is personal project of a music visualization tool that &amp;ldquo;forces&amp;rdquo; synesthesia by associating colors with notes. The main goal is to assist the music learning process, by attaching vision and audition.&lt;/p&gt;

&lt;p&gt;The ChromaJammer was written in the &lt;a href=&#34;http://puredata.info/&#34;&gt;PureData&lt;/a&gt; programming language. The patch gets the audio from an external source (mic or music player) and analyses which notes are being played. Each note has an associated color. The color scheme was based on the circle of fifths, since it arranges the notes in an interesting manner: the interval between adjacent notes is the most consonant possible. The circle of fifths is drawn alongside a representation of a guitar arm. When a note is identified, its respective frets lights up on the guitar arm.&lt;/p&gt;

&lt;p&gt;As an amateur musician, I&amp;rsquo;ve always wanted to improvise on the guitar, but struggled in learning music theory. On the other hand, I found myself with the right knowledge to develop a tool to enhance my capabilities. The project is open-source and can be downloaded from my &lt;a href=&#34;https://github.com/andresbrocco/ChromaJammer&#34;&gt;GitHub repository&lt;/a&gt;. I hope more people enjoy the tool, and maybe some day I&amp;rsquo;ll go back to this project.&lt;/p&gt;

&lt;p&gt;I have made a video presenting the patch and showing it in action!&lt;/p&gt;

&lt;video width=&#34;320&#34; height=&#34;240&#34; controls&gt;
  &lt;source src=&#34;https://andresbrocco.github.io/project/chroma-jammer/ChromaJammer_Demo.mp4&#34; type=&#34;video/mp4&#34;&gt;
Your browser does not support the video tag.
&lt;/video&gt; 
</description>
    </item>
    
  </channel>
</rss>
